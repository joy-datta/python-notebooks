{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative data, Nominal and Ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is often useful to measure objects in terms of quality not quantity. This qualitative infomation is often represented as an observation's membership in a discrete category such as gender, color, brand of car etc. Sets of categories with no intrinsic ordering is called nominal. Examples of nominal categories: 1. Red, Green, Blue 2. Man, Woman 3. Mango, Orange, Apple. When a set of categories has some natural ordering we refer to it as ordinal. Examples are: 1. Low, Medium and High 2. Young, Old 3. Agree, Neutral, Disagree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding nominal categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = np.array([\n",
    "    [\"Chittagong\"],\n",
    "    [\"Sylhet\"],\n",
    "    [\"Toronto\"],\n",
    "    [\"Toronto\"],\n",
    "    [\"Nashville\"],\n",
    "    [\"Singapore city\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chittagong', 'Nashville', 'Singapore city', 'Sylhet', 'Toronto'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use classes_ method to output the classes\n",
    "one_hot.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chittagong', 'Sylhet', 'Toronto', 'Toronto', 'Nashville',\n",
       "       'Singapore city'], dtype='<U14')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if we want to reverse the one hot encoding, we can use inverse_transform\n",
    "one_hot.inverse_transform(one_hot.transform(feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pandas for one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chittagong</th>\n",
       "      <th>Nashville</th>\n",
       "      <th>Singapore city</th>\n",
       "      <th>Sylhet</th>\n",
       "      <th>Toronto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chittagong  Nashville  Singapore city  Sylhet  Toronto\n",
       "0           1          0               0       0        0\n",
       "1           0          0               0       1        0\n",
       "2           0          0               0       0        1\n",
       "3           0          0               0       0        1\n",
       "4           0          1               0       0        0\n",
       "5           0          0               1       0        0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(feature[:, 0]) #create dummy variables for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one helpful ability of sklearn is to handle a situation where each observation lists multiple classes\n",
    "#create multiclass features\n",
    "multiclass_feature = np.array([\n",
    "    (\"Chittagong\", \"Sylhet\"),\n",
    "    (\"Singapore city\", \"Manilla\"),\n",
    "    (\"Toronto\", \"Montreal\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_multiclass = MultiLabelBinarizer() #create multiclass one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multiclass.fit_transform(multiclass_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chittagong', 'Manilla', 'Montreal', 'Singapore city', 'Sylhet',\n",
       "       'Toronto'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multiclass.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the classes have no intrinsic ordering, numerical values create an ordering errorneously that is not present. The proper strategy is to create a binary feature for each class in the original feature. This is called one hot encoding. It if often recommended that after one hot encoding a feature, we drop one of the one hot encoded features in the resulting matrix to avoid linear dependence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding ordinal categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pandas dataframe's replace method to transform string labels to numerical equivalents\n",
    "dataframe = pd.DataFrame({\"Score\": [\"Low\", \"Low\", \"Medium\", \"Medium\", \"Low\", \"Medium\", \"High\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mapper\n",
    "scale_mapper = {\"Low\": 1,\"Medium\": 2, \"High\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    2\n",
       "4    1\n",
       "5    2\n",
       "6    3\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace feature values with scale\n",
    "dataframe[\"Score\"].replace(scale_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#often we have a feature with classes that have some kind of natural ordering such as: strongly agree, agree, neutral, disagree, strongly disagree.\n",
    "#it is important that our choice of numeric values is based on our prior information on the ordianl classes\n",
    "#in our solution, high is literally three times larger than low\n",
    "#this is fine in any instances, but can break down if the assumed intervals between the classes are not equal\n",
    "dataframe = pd.DataFrame({\n",
    "    \"Score\": [\n",
    "        \"Low\", \"Low\", \"Medium\", \"Medium\", \"High\", \"Barely More Than Medium\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_mapper = {\"Low\": 1, \"Medium\": 2, \"Barely More Than Medium\": 3, \"High\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    2\n",
       "4    4\n",
       "5    3\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"Score\"].replace(scale_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this example, the distance between Low and Medium is the same as the distance between Medium and Barely More Than Medium\n",
    "#which is almost certainly not accurate\n",
    "#the best approach is to be conscious about the numerical values mapped to classes\n",
    "scale_mapper = {\"Low\": 1, \"Medium\": 2, \"Barely More Than Medium\": 2.1, \"High\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    2.0\n",
       "3    2.0\n",
       "4    3.0\n",
       "5    2.1\n",
       "Name: Score, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"Score\"].replace(scale_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding dictionaries of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = [\n",
    "    {\"Red\": 0, \"Green\": 0, \"Blue\": 0},\n",
    "    {\"Red\": 10, \"Green\": 0, \"Blue\": 20},\n",
    "    {\"Red\": 20, \"Green\": 20, \"Blue\": 120},\n",
    "    {\"Red\": 30, \"Green\": 10, \"Blue\": 10},\n",
    "    {\"Red\": 0, \"Green\": 50, \"Blue\": 0},\n",
    "    {\"Red\": 0, \"Green\": 10, \"Blue\": 30},\n",
    "    {\"Red\": 50, \"Green\": 10, \"Blue\": 0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dicitonary vectorizer\n",
    "dictvectorizer = DictVectorizer(sparse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dictionary to feature matrix\n",
    "features = dictvectorizer.fit_transform(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [ 20.,   0.,  10.],\n",
       "       [120.,  20.,  20.],\n",
       "       [ 10.,  10.,  30.],\n",
       "       [  0.,  50.,   0.],\n",
       "       [ 30.,  10.,   0.],\n",
       "       [  0.,  10.,  50.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by default DictVectorizer outputs a sparse matrix that only stores elements with a nonzero number. THis can be very helpful when we have massive matrices\n",
    "#and we want to minimize the memory requirements\n",
    "#we can force DictVectorizer to output a dense matrix using sparse = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Blue', 'Green', 'Red'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the feature names\n",
    "dictvectorizer.get_feature_names_out() #get_feature_names is deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blue</th>\n",
       "      <th>Green</th>\n",
       "      <th>Red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Blue  Green   Red\n",
       "0    0.0    0.0   0.0\n",
       "1   20.0    0.0  10.0\n",
       "2  120.0   20.0  20.0\n",
       "3   10.0   10.0  30.0\n",
       "4    0.0   50.0   0.0\n",
       "5   30.0   10.0   0.0\n",
       "6    0.0   10.0  50.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(features, columns = dictvectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a common situation when working with NLP. For example, we might have a collection of documents and for each document we have a dictionary containing the number of times every word appears in the document. Using DictVectorizer, we can easily create a feature matrix where every feature is the number of times a word appears in each document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create word count dictionaries for five documents\n",
    "doc_1_word_count = {\"A\": 10, \"As\": 50, \"Are\": 10, \"Be\": 30, \"Biscuits\": 10}\n",
    "doc_2_word_count = {\"A\": 0, \"As\": 20, \"Are\": 10, \"Be\": 30, \"Biscuits\": 50}\n",
    "doc_3_word_count = {\"A\": 40, \"As\": 30, \"Are\": 10, \"Be\": 20, \"Biscuits\": 0}\n",
    "doc_4_word_count = {\"A\": 10, \"As\": 20, \"Are\": 30, \"Be\": 30, \"Biscuits\": 50}\n",
    "doc_5_word_count = {\"A\": 50, \"As\": 50, \"Are\": 10, \"Be\": 0, \"Biscuits\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list\n",
    "doc_word_counts = [doc_1_word_count, doc_2_word_count, doc_3_word_count, doc_4_word_count, doc_5_word_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10., 10., 50., 30., 10.],\n",
       "       [ 0., 10., 20., 30., 50.],\n",
       "       [40., 10., 30., 20.,  0.],\n",
       "       [10., 30., 20., 30., 50.],\n",
       "       [50., 10., 50.,  0.,  0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert list of word count dictionaries into feature matrix\n",
    "dictvectorizer.fit_transform(doc_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is just a toy example where there are only five unique words, so there are only five features in our matrix\n",
    "#you can imagine that if each document was actually a book in a university library our feature matrix would be very large\n",
    "#so we want to set sparse to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing missing class values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppose you have a categorical feature containing missing values that you want to replace with predicted values\n",
    "#the ideal solution is to train a ML classifier algorithm to predict the missing values\n",
    "#commonly a knn classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a feature matrix with categorical feature\n",
    "X = np.array([\n",
    "    [1, 1.2, 20.2],\n",
    "    [1, 10.2, 2.6],\n",
    "    [0, 1.7, 2.6],\n",
    "    [1, 53.5, 3.2]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature matrix with missing values in the categorical feature\n",
    "X_with_nan = np.array([\n",
    "    [np.nan, 20.1, 20.1],\n",
    "    [np.nan, 20.3, 2.1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train knn learner\n",
    "clf = KNeighborsClassifier(3, weights = \"distance\")\n",
    "trained_model = clf.fit(X[:, 1:], X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict missing values' class\n",
    "imputed_values = trained_model.predict(X_with_nan[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join column of predicted class with their other features\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1, 1), X_with_nan[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , 20.1, 20.1],\n",
       "       [ 1. , 20.3,  2.1]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_with_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , 20.1, 20.1],\n",
       "       [ 1. , 20.3,  2.1],\n",
       "       [ 1. ,  1.2, 20.2],\n",
       "       [ 1. , 10.2,  2.6],\n",
       "       [ 0. ,  1.7,  2.6],\n",
       "       [ 1. , 53.5,  3.2]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join two feature matrices\n",
    "np.vstack((X_with_imputed, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an alternative solution is to fill in missing values with the features' most frequent value\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_complete = np.vstack((X_with_nan, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan, 20.1, 20.1],\n",
       "       [ nan, 20.3,  2.1],\n",
       "       [ 1. ,  1.2, 20.2],\n",
       "       [ 1. , 10.2,  2.6],\n",
       "       [ 0. ,  1.7,  2.6],\n",
       "       [ 1. , 53.5,  3.2]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , 20.1, 20.1],\n",
       "       [ 1. , 20.3,  2.1],\n",
       "       [ 1. ,  1.2, 20.2],\n",
       "       [ 1. , 10.2,  2.6],\n",
       "       [ 0. ,  1.7,  2.6],\n",
       "       [ 1. , 53.5,  3.2]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = SimpleImputer(strategy = \"most_frequent\")\n",
    "imputer.fit_transform(X_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is recommended to include a binary feature indicating which observations contain imputed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose you have a target vector with highly imbalanced classes. Collect more data. If that isn't possible, change the metrics used to evaluate your model. If that doesn't work, consider using a model's built-in class weight parameters, downsampling or upsampling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "feature = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove first 40 observations\n",
    "features = features[40:, :]\n",
    "target = target[40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create binary target vector indicating if class 0\n",
    "target = np.where((target == 0), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target #imbalanced target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 0.9, 1: 0.1})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#many algorithms in sklearn offer a parameter to weight classes during training to counteract the effect of their imbalance\n",
    "#RandomForestClassifier includes a class_weight parameter\n",
    "weights = {0: 0.9, 1: 0.1}\n",
    "#create random forest classifier with weights\n",
    "RandomForestClassifier(class_weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#or you can pass balanced to automatically create weights inversely propotional to class frequencies\n",
    "RandomForestClassifier(class_weight = \"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way is downsampling the majority class or upsample the minority class\n",
    "#indices of each class' observations\n",
    "i_class0 = np.where(target == 0)[0]\n",
    "i_class1 = np.where(target == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of observations in each class\n",
    "n_class0 = len(i_class0)\n",
    "n_class1 = len(i_class1)\n",
    "n_class0, n_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every observation of class 0, randomly sample\n",
    "#from class 1 without replacement\n",
    "i_class1_downsampled = np.random.choice(i_class1, size = n_class0, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join together class 0's target vector with the downsampled class 1's target vector\n",
    "np.hstack((target[i_class0], target[i_class1_downsampled]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join together class 0's feature matrix with the downsampled class 1's feature matrix\n",
    "#np.vstack((features[i_class0, :], features[i_class1_downsampled, :]))[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every observation in class 1, randomly sample from class 0 with replacement\n",
    "i_class0_upsampled = np.random.choice(i_class0, size = n_class1, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(i_class0_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 5, 0, 6, 8, 5, 6, 8, 6, 0, 6, 7, 6, 6, 2, 8, 8, 1, 5, 6, 6,\n",
       "       6, 2, 1, 1, 1, 3, 2, 9, 2, 4, 6, 3, 9, 1, 9, 1, 1, 4, 7, 9, 4, 6,\n",
       "       7, 1, 2, 9, 2, 1, 7, 8, 4, 3, 1, 7, 5, 5, 5, 8, 1, 8, 5, 4, 5, 5,\n",
       "       9, 7, 4, 5, 1, 6, 5, 4, 0, 5, 7, 3, 0, 9, 4, 3, 3, 6, 7, 6, 2, 2,\n",
       "       4, 5, 2, 2, 5, 3, 4, 6, 5, 5, 7, 8], dtype=int64)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_class0_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join together class 0's upsampled target vector with class 1's target vector\n",
    "np.concatenate((target[i_class0_upsampled], target[i_class1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join together class 0's upsampled feature matrix with class 1's feature matrix\n",
    "#np.vstack((features[i_class0_upsampled, :], features[i_class1, :]))[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
